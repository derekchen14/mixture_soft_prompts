import copy
import math
import os, pdb, sys
import re
import json
import torch
import numpy as np
import pandas as pd
import random

from torch.cuda.amp import autocast
from collections import Counter, defaultdict
from transformers import AutoTokenizer, GPT2LMHeadModel
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from tqdm import tqdm as progress_bar
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

from assets.static_vars import CHECKPOINTS, device, debug_break, accelerator
from components.logger import TextEvaluationLogger
from utils.process import get_dataloader

def postprocess_outputs(args, raw_outputs, all_inputs):
  """accepts list of raw outputs generated by the model and
  returns a list of post-processed strings for comparison with the target"""
  size = len(raw_outputs)
  assert(size == len(all_inputs))
  ending_tokens = ['<pad>', '</s>', '<|endoftext|>']
  if args.task == 'in_context':
    ending_tokens.extend(['.', '[SEP]', '[PAD]', '\n'])
  if args.task == 'synthesize':
    ending_tokens.append(' | ')

  parsed_preds = []
  for out, inp in zip(raw_outputs, all_inputs):

    # Remove prefix input sequence
    if args.model == 'gpt':
      pred = out[len(inp):]
    elif out.startswith('<pad> '):
      pred = out[6:]
    elif out.startswith('<extra_id_0> '):
      pred = out[13:]
    else:
      pred = out

    prediction = pred
    if args.task != 'in_context':
      prediction = pred.replace('\n', ' | ')

    # Truncate suffix by EOS marker
    eos_index = len(prediction)
    for tok in ending_tokens:
      cand_index = prediction.find(tok)  # returns -1 if not found
      if 0 < cand_index and cand_index < eos_index:
        eos_index = cand_index
    pred_str = prediction[:eos_index]

    if args.task == 'synthesize':
      parsed_preds.append(pred_str)
      continue

    # Parse label by dataset
    parsed_list = parse_prediction(args.model, args.dataset, args.task, pred_str)
    if args.task == 'synthesize':
      max_tokens = args.target_max_len
    elif args.dataset == 'topv2':
      max_tokens = 32
    else:
      max_tokens = 7
    parsed_pred = [normalize_text(pl, end=max_tokens) for pl in parsed_list]
    parsed_preds.append(parsed_pred)

  return parsed_preds

def parse_prediction(model, dataset, task_name, pred_string):
  """ Takes pred string as part of post-procesing
  Returns parsed result as a list of predictions according to each dataset and task
  """
  if task_name == 'in_context' and model != 'api':  # in-context string has no special tokens
    separator, label_sep = ';', ':'
  elif dataset == 'actdial':
    separator, label_sep = ';', 'Acts are'
  elif dataset == 'nlu++':
    separator, label_sep = ";", '<label>'
  else:  # crossner and topv2
    separator, label_sep = '<sep>', '<label>'

  if model == 'godel' and task_name == 'soft_prompt': 
    separator = separator + "|<extra_id_\d+>"
    parsed_list = re.split(separator, pred_string)
    return parsed_list

  parsed_list = pred_string.split(separator)
  return parsed_list

def calculate_token_f1(predict_tokens, target_tokens):
  match, possible_matches = 0, 0.0001
  found, possible_found = 0, 0.0001

  for pt in predict_tokens:
    if pt in target_tokens:
      match += 1
    possible_matches += 1

  for tt in target_tokens:
    if tt in predict_tokens:
      found += 1
    possible_found += 1

  precision = round(match/possible_matches, 3)
  recall = round(found/possible_found, 3)
  return precision, recall

def calculate_nlu_f1(general_candidates, predict_tokens, target_tokens):
  target_general_tokens = [tt.strip() for tt in target_tokens if tt.strip() in general_candidates]
  predict_general_tokens = [pt.strip() for pt in predict_tokens if pt.strip() in general_candidates]

  prec, recall = calculate_token_f1(predict_general_tokens, target_general_tokens)
  return prec, recall

def overlap(numerator_tokens, denominator_tokens, use_max=False):
  '''
  calculate the percentage of the overlap between single predicted entity and single target entity.
  Args:
    numerator_tokens [list]: single predict/target entity string split by space
    denominator_tokens [list]: single target/predict entity string split by space

  Returns:
    the percentage of overlap based on the target entity
  '''
  overlap_token = [token for token in numerator_tokens if token in denominator_tokens]
  total = max(len(numerator_tokens), len(denominator_tokens)) if use_max else len(numerator_tokens)
  score = len(overlap_token) / total
  return score

def calculate_ner_f1(predict_tokens, target_tokens):
  '''
    calculate the precision and recall score for crossner dataset each sample
    Args:
      predict_token [list]: predict entity string split by ";"
      target_token [list]: target entity string split by ";"

    Returns:
      precision:
      recall:
    '''
  match, possible_matches = 0, 0.0001
  found, possible_found = 0, 0.0001

  predict_dict = defaultdict(list)
  target_dict = defaultdict(list)

  for pt in predict_tokens:
    # This is for "[ ]" around the entity_name
    if len(pt.split("[")) != 2:
      possible_matches += 1
      continue
    entity = normalize_text(pt.split("[")[0].strip())
    name = normalize_text(pt.split("[")[1].strip("[ ]"))
    predict_dict[name].append(entity)
    possible_matches += 1

  for tt in target_tokens:
    # This is for "[ ]" around the entity_name
    if len(tt.split("[")) != 2:
      possible_found += 1
      continue
    entity = normalize_text(tt.split("[")[0].strip())
    name = normalize_text(tt.split("[")[1].strip("[ ]"))
    target_dict[name].append(entity)
    possible_found += 1

  predict_dict_init = copy.deepcopy(predict_dict)
  target_dict_init = copy.deepcopy(target_dict)

  for name, entities in predict_dict.items():
    if not target_dict.get(name):
      continue
    for pred_entity in entities:
      pred_token = pred_entity.split(" ")
      for target_entity in target_dict_init.get(name):
        target_token = target_entity.split(" ")
        if overlap(target_token, pred_token) == 1.0:
          match += 1
          target_dict_init.get(name).remove(target_entity)
          break

  for name, entities in target_dict.items():
    if not predict_dict.get(name):
      continue
    for target_entity in entities:
      target_token = target_entity.split(" ")
      for pred_entity in predict_dict_init.get(name):
        pred_token = pred_entity.split(" ")
        if overlap(pred_token, target_token) == 1.0:
          found += 1
          predict_dict_init.get(name).remove(pred_entity)
          break

  precision = round(match / possible_matches, 3)
  recall = round(found / possible_found, 3)
  return precision, recall

def calculate_accuracy(predict_tokens, target_tokens):
  '''
    calculate the exact_acc, intents_acc and slots_acc score for topv2 dataset each sample
    Args:
      predict_token [list]: predict intents and slots string split by "<sep>"
      target_token [list]: target intent and slots string split by "<sep>"
    '''
  exact_acc = [0,0]
  intents_acc = 0
  slots_acc = 0
  if len(predict_tokens) != 2:
    return np.mean(exact_acc), intents_acc, slots_acc

  pred_intents = [intent.strip().lower() for intent in predict_tokens[0].split(",")]
  target_intents = [intent.strip().lower() for intent in target_tokens[0].split(",")]
  intents_acc = overlap(pred_intents, target_intents, use_max=True)
  if intents_acc == 1:
    exact_acc[0] = 1

  pred_slots = predict_tokens[1].strip().lower().split(",")
  pred_slots_dict = defaultdict(list)
  possible_slots_num = 0
  slots_match = 0
  for pair in pred_slots:
    if pair:
      possible_slots_num += 1
    if len(pair.split(" [")) != 2:
      continue
    slot_name, slot_value = pair.split(" [")[0].strip("[ ]"), pair.split(" [")[1].strip("[ ]")
    pred_slots_dict[slot_name].append(slot_value)

  target_slots = target_tokens[1].strip().lower().split(",")
  target_slots_dict = defaultdict(list)
  target_slots_num = 0
  for pair in target_slots:
    if pair:
      target_slots_num += 1
    if len(pair.split(" [")) != 2:
      continue
    slot_name, slot_value = pair.split(" [")[0].strip("[ ]"), pair.split(" [")[1].strip("[ ]")
    target_slots_dict[slot_name].append(slot_value)

  if not possible_slots_num and not target_slots_num:
    exact_acc[1] = 1
    slots_acc = 1
  elif not possible_slots_num:
    exact_acc[1] = 0
    slots_acc = 0
  else:
    for target_name, target_values in target_slots_dict.items():
      pred_values = pred_slots_dict.get(target_name)
      if not pred_values:
        exact_acc[1] = 0
        continue
      if overlap(pred_values, target_values, use_max=True):
        slots_match += 1
    slots_acc = slots_match / max(possible_slots_num, target_slots_num)
    if slots_acc == 1:
      exact_acc[1] = 1

  return np.mean(exact_acc), intents_acc, slots_acc

re_art = re.compile(r'\b(a|an|the)\b')
# Skip over comma (,) left bracket (\[) and right bracket (\])
re_punc = re.compile(r'[!"#$%&()\*\+\-./:;=?@\\^`{|}~_\']')

def normalize_text(s, end=7):
  # Lower text and remove punctuation, articles and extra whitespace.
  if s in ['none', '<none>', 'remove', '<remove>']:
    return '<none>'
  s = s.lower().strip()
  s = re_punc.sub(' ', s)
  s = re_art.sub(' ', s)
  parts = s.split()
  s = ' '.join(parts[:end])
  return s

def eval_quantify(args, predictions, targets, exp_logger):
  results = {'epoch': exp_logger.epoch}
  precisions, recalls = [], []
  error_samples = []
  exact_accuracies, intent_accuracies, slot_accuracies = [], [], []
  sep_token = '<sep>' if args.dataset in ['crossner', 'topv2'] and \
                         (args.task != 'in_context' or args.model == "api") else ';'

  if args.dataset == 'nlu++':
    ont_path = os.path.join('assets', 'nlu++', 'ontology.json')
    ontology = json.load(open(ont_path, 'r'))
    general_candidates = [normalize_text(cand) for cand in ontology["general"]["intents"].keys()]

  for pred_list, target in zip(predictions, targets):
    # predictions are expected to be parsed already from postprocess_outputs
    assert(isinstance(pred_list, list))

    if args.dataset == 'topv2':
      target_list = [normalize_text(t, end=32) for t in target.split(sep_token)]
      exact_acc, intent_acc, slot_acc = calculate_accuracy(pred_list, target_list)
      intent_accuracies.append(intent_acc)
      slot_accuracies.append(slot_acc)
      exact_accuracies.append(exact_acc)

      if intent_acc < 0.5 or slot_acc < 0.5:
        error_dict = {"predict list": pred_list, "target list": target_list,
                      "intent accuracy": intent_acc, "slot accuracy": slot_acc,
                      "exact accuracy": exact_acc}
        error_samples.append(error_dict)

    else:
      target_list = [normalize_text(t) for t in target.split(sep_token)]
      if args.dataset == 'crossner':
        prec, rec = calculate_ner_f1(pred_list, target_list)
      elif args.dataset == 'nlu++':
        if args.setting == 'cross':
          prec, rec = calculate_nlu_f1(general_candidates, pred_list, target_list)
        else:  # k-fold
          prec, rec = calculate_token_f1(pred_list, target_list)
      else:
        prec, rec = calculate_token_f1(pred_list, target_list)
      precisions.append(prec)
      recalls.append(rec)
      if prec < 0.5 or rec < 0.5:
        error_dict = {"predict list": pred_list, "target list": target_list,
                      "precision": prec, "recall": rec}
        error_samples.append(error_dict)

  if args.verbose and args.do_eval:
    size = min(10, len(error_samples))
    select_errors = np.random.choice(error_samples, size, replace=False)
    print(json.dumps(select_errors.tolist(), sort_keys=False, indent=4))

  if args.debug and args.verbose:
    exp_logger.save_eval_result(predictions, targets, precisions, recalls)

  if args.dataset == 'topv2':
    results['intents_acc'] = round(np.mean(intent_accuracies), 4)
    results['slots_acc'] = round(np.mean(slot_accuracies), 4)
    results['accuracy'] = round(np.mean(exact_accuracies), 4)
  else:
    results = store_f1(results, precisions, recalls)

  exp_logger.log_info(results)
  return results

def store_f1(results, precisions, recalls):
  precision = round(np.mean(precisions), 4)
  results['precision'] = precision
  recall = round(np.mean(recalls), 4)
  results['recall'] = recall
  f1_score = 2 * ((precision*recall) / (precision+recall))
  results['f1_score'] = round(f1_score, 4)
  return results

def synthesize_eval(args, predictions, targets, exp_logger):
  results = {'epoch': exp_logger.epoch}
  precisions, recalls, scores = [], [], []

  for prediction, target in zip(predictions, targets):
    pred = prediction.split()
    targ = target.split()

    precision, recall = calculate_token_f1(pred, targ)
    precisions.append(precision)
    recalls.append(recall)

    reference = [targ]  # transform into a a list of lists
    weights = (0.25, 0.25, 0.25, 0.25)
    smoother = SmoothingFunction().method3
    bleu_score = sentence_bleu(reference, pred, weights, smoother)
    scores.append(bleu_score)

  results = store_f1(results, precisions, recalls)
  results['bleu'] = round(np.mean(scores), 4)
  exp_logger.log_info(results)
  return results

def eval_qualify(args, inputs, outputs, targets, parsed=[]):
  include_parse = len(parsed) < 0

  if len(outputs) <= 7:
    positions = range(len(outputs))
  else:
    positions = random.sample(range(len(outputs)), 7)

  for idx, pos in enumerate(positions):
    nl_input, raw_output, nl_target = inputs[pos], outputs[pos], targets[pos]
    if args.model == 'gpt':
      history_size = len(nl_input)
      raw_output = raw_output[history_size:].replace('<pad> ', '')
      nl_input = nl_input.replace('<pad> ', '')
    if args.method == 'msp':
      nl_input = nl_input.replace('placeholder', '').replace('attribute_token', '|')

    print(f"{idx+1}) {nl_input}")
    print(f'  Prediction: {raw_output}')
    if include_parse:
      parsed_pred = parsed[pos]
      print(f'  Parsed: {parsed_pred}')
    print(f'  Target: {nl_target}') # \n

def multi_label_classification(predictions, y_trues, threshold=0.5):
    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
    sigmoid = torch.nn.Sigmoid()
    y_preds = []
    for prediction in predictions:
      probs = sigmoid(torch.tensor(prediction)).numpy()
      prediction = np.zeros(probs.shape)
      prediction[np.where(probs >= threshold)] = 1
      y_preds.append(prediction)

    # then, compute metrics
    metrics = {
      'accuracy': round(precision_score(y_trues, y_preds, average='micro'), 3),
      'f1_score': round(f1_score(y_trues, y_preds, average='micro'), 3),
      'recall': round(recall_score(y_trues, y_preds, average='micro'), 3),
      'precision': round(precision_score(y_trues, y_preds, average='micro'), 3),
      'accuracy_raw': round(accuracy_score(y_trues, y_preds), 3)
    }
    return metrics


def multi_label_classification_crossner(predictions, y_trues, threshold=0.5):
  # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
  sigmoid = torch.nn.Sigmoid()
  y_preds = []
  for prediction in predictions:
    probs = sigmoid(torch.tensor(prediction)).numpy()
    prediction = np.zeros(probs.shape)
    prediction[np.where(probs >= threshold)] = 1
    y_preds.append(prediction)

  # then, compute metrics
  metrics = {
    'accuracy': round((precision_score(y_trues, y_preds, average='micro') + 1) / 2, 3),
    'f1_score': round(f1_score(y_trues, y_preds, average='micro'), 3),
    'recall': round(recall_score(y_trues, y_preds, average='micro'), 3),
    'precision': round((precision_score(y_trues, y_preds, average='micro') + 1) / 2, 3),
    'accuracy_raw': round(accuracy_score(y_trues, y_preds), 3)
  }
  return metrics


def run_openai_eval(args, all_inputs, all_outputs, all_targets, exp_logger):
  parsed_preds = postprocess_outputs(args, all_outputs, all_inputs)
  if args.qualify and len(all_outputs) > 0:
    results = eval_qualify(args, all_inputs, all_outputs, all_targets, parsed_preds)
  if args.quantify:
    results = eval_quantify(args, parsed_preds, all_targets, exp_logger)
  return results


def run_eval(args, model, dataset, exp_logger, split='dev', attribute_embeddings=None):
  tokenizer = dataset.tokenizer
  dataloader = get_dataloader(args, dataset, split)
  num_batches = debug_break if args.debug else len(dataloader)
  our_method = (args.method == 'msp') and (args.task == 'synthesize')
  exp_logger.start_eval(num_batches)
  model.eval()

  '''goes through model generation without backprop, rather than classification '''
  all_inputs, all_outputs, all_targets  = [], [], []
  for batch in progress_bar(dataloader, total=len(dataloader)):
    if our_method:
      inputs, targets, metadata = dataset.collate(args, batch)
      attribute_embeddings.set_constraints(metadata)
    else:
      inputs, targets = dataset.collate(args, batch)
      inputs = inputs.to(device)

    with torch.no_grad():
      # defaults to greedy sampling, for param details see https://huggingface.co/docs/transformers/
      #        v4.15.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate
      outputs = model.generate(**inputs,
                    max_new_tokens=args.target_max_len, early_stopping=True,
                    repetition_penalty=args.threshold, temperature=args.temperature)

    sst = args.dataset in ['nlu++', 'actdial', 'banking'] or args.task == 'synthesize'

    inps = inputs['input_ids']
    if args.task == 'soft_prompt' or our_method:
      input_strings = tokenizer.batch_decode(inps[:, args.n_tokens:], skip_special_tokens=sst)
    else:
      input_strings = tokenizer.batch_decode(inps, skip_special_tokens=sst)

    output_strings = tokenizer.batch_decode(outputs.detach(), skip_special_tokens=sst)
    all_inputs.extend(input_strings)
    all_outputs.extend(output_strings)
    all_targets.extend(targets)

    exp_logger.eval_step += 1
    if args.debug and exp_logger.eval_step >= debug_break: break
    if exp_logger.eval_step > 400 and args.method == 'msp': break

  parsed_preds = postprocess_outputs(args, all_outputs, all_inputs)
  if args.qualify and len(all_outputs) > 0:
    results = eval_qualify(args, all_inputs, all_outputs, all_targets, parsed_preds)
  if args.quantify:
    if args.task == 'synthesize':
      results = synthesize_eval(args, parsed_preds, all_targets, exp_logger)
    else:
      results = eval_quantify(args, parsed_preds, all_targets, exp_logger)
  return results

def accelerated_eval(args, model, dataset, exp_logger, split='dev', attribute_embeddings=None):
  tokenizer = dataset.tokenizer
  dataloader = get_dataloader(args, dataset, split)
  num_batches = debug_break if args.debug else len(dataloader)
  exp_logger.start_eval(num_batches)
  model.eval()

  if args.do_eval:
    dataloader, model = accelerator.prepare(dataloader, model)
  disabled = not accelerator.is_local_main_process

  '''goes through model generation without backprop, rather than classification '''
  all_inputs, all_outputs, all_targets  = [], [], []
  for batch in progress_bar(dataloader, total=len(dataloader), disable=disabled):
    if args.method == 'msp':
      inputs, targets, metadata = dataset.collate(args, batch)
      attribute_embeddings.set_constraints(metadata)
    else:
      inputs, targets = dataset.collate(args, batch)
      inputs = inputs.to(device)

    with autocast(dtype=torch.float16):
      with torch.no_grad():
        # defaults to greedy sampling, for param details see https://huggingface.co/docs/transformers/
        #        v4.15.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate
        outputs = model.generate(**inputs, max_new_tokens=args.target_max_len, early_stopping=True,
                        repetition_penalty=args.threshold, temperature=args.temperature)

    inputs, outputs, targets = accelerator.gather_for_metrics((inputs, outputs, targets))
    sst = args.dataset in ['nlu++', 'actdial', 'banking']

    inps = inputs['input_ids']
    if args.model == 'godel' and args.task == 'soft_prompt':
      inps[inps == -1] = 0

    input_strings = tokenizer.batch_decode(inps, skip_special_tokens=sst)
    output_strings = tokenizer.batch_decode(outputs.detach(), skip_special_tokens=sst)
    target_strings = dataset.tensor_to_target(targets)

    all_inputs.extend(input_strings)
    all_outputs.extend(output_strings)
    all_targets.extend(target_strings)

    exp_logger.eval_step += 1
    if args.debug and exp_logger.eval_step >= debug_break: break

  if accelerator.is_main_process:
    parsed_preds = postprocess_outputs(args, all_outputs, all_inputs)
    if args.qualify:
      results = eval_qualify(args, all_inputs, all_outputs, all_targets, parsed_preds)
    if args.quantify:
      if args.task == 'synthesize':
        results = synthesize_eval(args, parsed_preds, all_targets, exp_logger)
      else:
        results = eval_quantify(args, parsed_preds, all_targets, exp_logger)
    return results

# ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  #
#     Methods for automatic evaluation and training the correctness classifier    #
# ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  #

def classify_eval(args, model, dataset, exp_logger, split='dev'):
  dataloader = get_dataloader(args, dataset, split)
  num_batches = debug_break if args.debug else len(dataloader)
  exp_logger.start_eval(num_batches)
  epoch = exp_logger.epoch

  if hasattr(model, 'name') and model.name == 'dual-classify':
    results = dual_classify(args, model, dataset, dataloader, epoch)
  else:
    results = single_classify(args, model, dataset, dataloader, epoch)
  exp_logger.log_info(results)
  return results

def single_classify(args, model, dataset, dataloader, epoch):
  all_outputs, all_targets  = [], []

  for batch in progress_bar(dataloader, total=len(dataloader)):
    inputs, targets = dataset.collate(args, batch)
    with torch.no_grad():
      output = model(**inputs)

    all_outputs.append(output.logits.detach().cpu().numpy())
    all_targets.extend(targets.detach().cpu().numpy())

  results = {'epoch': epoch}
  if args.dataset == "crossner":
    results.update(
      multi_label_classification_crossner(np.concatenate(all_outputs), all_targets, args.sig_threshold)
    )

  else:
    results.update(
       multi_label_classification(np.concatenate(all_outputs), all_targets, args.sig_threshold)
    )
  return results

def dual_classify(args, model, dataset, dataloader, epoch):
  intent_outputs, slot_outputs, intent_targets, slot_targets = [], [], [], []

  for batch in progress_bar(dataloader, total=len(dataloader)):
    inputs, targets = dataset.collate(args, batch)
    with torch.no_grad():
      output, _ = model(inputs, targets)
    intent_outputs.append(output["intent"].detach().cpu().numpy())
    slot_outputs.append(output["slot"].detach().cpu().numpy())
    intent_targets.extend(targets["intent"].detach().cpu().numpy())
    slot_targets.extend(targets["slot"].detach().cpu().numpy())

  intent_res = multi_label_classification(np.concatenate(intent_outputs), intent_targets, args.sig_threshold)
  slot_res = multi_label_classification(np.concatenate(slot_outputs), slot_targets, args.sig_threshold)
  results = {
    'epoch': epoch,
    'f1_score': intent_res['f1_score'],
    'accuracy': intent_res['accuracy'],
    'intents_acc': intent_res['accuracy'],
    'intent_f1': intent_res['f1_score'],
    'slots_acc': slot_res['accuracy'],
    'slot_f1': slot_res['f1_score'],
    'intent_accuracy_raw': intent_res['accuracy_raw'],
    'slot_accuracy_raw': slot_res['accuracy_raw']
  }
  return results

def eval_inter_distinctness(outputs, others):
  others_ngram_db = []
  for other_dict in others:
    unigrams = other_dict['text'].split()
    bigrams = []
    for i in range(len(unigrams) - 1):
      bigrams.append(f"{unigrams[i]}_{unigrams[i+1]}")
    trigrams = []
    for i in range(len(unigrams) - 2):
      trigrams.extend(f"{unigrams[i]}_{unigrams[i+1]}_{unigrams[i+2]}")
    others_ngram_db.append({
      'text': other_dict['text'],
      'bigrams': bigrams,
    })

  bigram_ols = []
  for output in progress_bar(outputs, total=len(outputs), desc='Finding overlap with others'):
    o_unigrams = output['text'].split()
    o_bigrams = []
    for i in range(len(o_unigrams) - 1):
      o_bigrams.append(f"{o_unigrams[i]}_{o_unigrams[i+1]}")
    top_other_bigram_ol = None
    has_bigrams = len(set(o_bigrams)) > 0
    for other in others_ngram_db:
      if not has_bigrams:
        continue
      bigram_overlap = len(set(other['bigrams']) & set(o_bigrams)) / len(set(o_bigrams))
      if top_other_bigram_ol is not None and bigram_overlap <= top_other_bigram_ol:
        continue
      top_other_bigram_ol = bigram_overlap
    if top_other_bigram_ol is not None:
      bigram_ols.append(top_other_bigram_ol)
  return {
    'overlap@2': round(sum(bigram_ols) / len(bigram_ols), 3),
  }

def eval_distinctness(outputs):
  """
  Produces distinct @ k for each prompt
  Modified from: https://github.com/alisawuffles/DExperts/blob/main/scripts/evaluation/evaluate_generations.py#L36-L56
  Args:
      outputs (list[dict]): list of text outputs generated by the model
  Returns:
      dict: distinct@k's
  """
  dist1, dist2, dist3 = [], [], []
  unigrams, bigrams, trigrams = [], [], []
  for output in progress_bar(outputs, total=len(outputs), desc='Evaluating k-distinctness'):
    o = output['text'].split(' ')
    unigrams.extend(o)
    for i in range(len(o) - 1):
        bigrams.append(o[i] + '_' + o[i+1])
    for i in range(len(o) - 2):
        trigrams.append(o[i] + '_' + o[i+1] + '_' + o[i+2])
  dist1 = None if len(unigrams) == 0 else len(set(unigrams)) / len(unigrams)
  dist2 = None if len(bigrams) == 0 else len(set(bigrams)) / len(bigrams)
  dist3 = None if len(trigrams) == 0 else len(set(trigrams)) / len(trigrams)

  # take the mean across prompts
  return {
    'distinct@1': round(np.nanmean(dist1), 3) if dist1 is not None else dist1,
    'distinct@2': round(np.nanmean(dist2), 3) if dist2 is not None else dist2,
    'distinct@3': round(np.nanmean(dist3), 3) if dist3 is not None else dist3
  }

def eval_perplexity(args, dataset):
  """
  Produces perplexity measure for a given set of outputs and prompts.
  Uses the model defined in ppl_model and ppl_tokenier
  Modified from: https://github.com/alisawuffles/DExperts/blob/main/scripts/
                                    evaluation/evaluate_generations.py#L16-L33
  Args:
      outputs (list[dict]): list of text outputs generated by the model
      ppl_model: the model used to evaluate perplexity
      ppl_tokenier: the tokenier to use for that model.
  Returns:
      dict: dict containing perplexity score
  """
  ckpt_name = 'gpt2-large'
  model = GPT2LMHeadModel.from_pretrained(ckpt_name).to(device)
  tokenizer = AutoTokenizer.from_pretrained(ckpt_name)
  tokenizer.pad_token = tokenizer.eos_token
  dataloader = get_dataloader(args, dataset, split='train')

  perplexities = []
  for batch in progress_bar(dataloader, total=len(dataloader), desc='Evaluating fluency'):
    if args.dataset == "topv2":
      full_input_ids, targets = dataset.collate(args, batch, tokenizer)
    else:
      full_input_ids, targets, lexical_match = dataset.collate(args, batch, tokenizer)
    if full_input_ids is None or full_input_ids['input_ids'].shape[1] == 0:
      continue
    
    pad_id = tokenizer.pad_token_id
    fii = full_input_ids['input_ids']
    seq_len = fii.shape[1]
    # replace labels with with -100
    targets = torch.where(fii == pad_id, -100, fii)
    loss = model(**full_input_ids, labels=targets)[0] / seq_len
    ppl = math.exp(loss.item())

    perplexities.append(ppl)
  return np.nanmean(perplexities)

def eval_correctness(args, dataset, classifier):
  dataloader = get_dataloader(args, dataset, split='train')

  if hasattr(classifier, 'name') and classifier.name == "dual-classify":
    all_intents_outputs, all_slots_outputs, all_intents_targets, all_slots_targets = [], [], [], []

    for batch in progress_bar(dataloader, total=len(dataloader)):
      inputs, targets = dataset.collate(args, batch)
      if inputs is None: continue
      output, _ = classifier(inputs, targets)
      all_intents_outputs.append(output["intent"].detach().cpu().numpy())
      all_slots_outputs.append(output["slot"].detach().cpu().numpy())
      all_intents_targets.extend(targets["intent"].detach().cpu().numpy())
      all_slots_targets.extend(targets["slot"].detach().cpu().numpy())

    intents_result = multi_label_classification(np.concatenate(all_intents_outputs), all_intents_targets,
                                                args.sig_threshold)
    slots_result = multi_label_classification(np.concatenate(all_slots_outputs), all_slots_targets,
                                              args.sig_threshold)
    correct_score = {
      'f1_score': intents_result['f1_score'],
      'accuracy': intents_result['accuracy'],
      'intents_acc': intents_result['accuracy'],
      'intent_f1': intents_result['f1_score'],
      'slots_acc': slots_result['accuracy'],
      'slot_f1': slots_result['f1_score'],
      'intent_accuracy_raw': intents_result['accuracy_raw'],
      'slot_accuracy_raw': slots_result['accuracy_raw']
    }

  else:
    all_targets, all_outputs, all_matches = [], [], []

    for batch in progress_bar(dataloader, total=len(dataloader), desc='Evaluating correctness'):
      inputs, targets, lex_match = dataset.collate(args, batch)
      if inputs is None:
        continue
      outputs = classifier(**inputs)
      all_outputs.append(outputs.logits.detach().cpu().numpy())
      all_targets.append(targets.detach().cpu().numpy())
      all_matches.extend(lex_match)

    correct_score = {'accuracy': 1.0}
    if len(all_outputs) != 0: 
      correct_score = multi_label_classification(np.concatenate(all_outputs), np.concatenate(all_targets),
                                               args.sig_threshold)
  result = {'correctness': correct_score}
  if args.dataset == 'crossner':
    lex_score = np.average(all_matches)
    result['lex_score'] = round(lex_score, 3)
    result['final_score'] = round((correct_score['accuracy'] + lex_score) / 2, 3)
  return result

